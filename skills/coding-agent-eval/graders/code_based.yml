# Code-Based Grader Configuration
# 完全基于代码执行结果，不依赖 LLM 理解

grader_type: "code_based"
description: "Execute tests and static analysis to determine pass/fail"

rules:
  test_execution:
    - name: "Run unit tests"
      command: "pytest {test_files} -v --tb=short"
      timeout: 300
      pass_condition: "exit_code == 0"
    
    - name: "Run integration tests"
      command: "pytest tests/integration -v"
      timeout: 600
      pass_condition: "exit_code == 0"
  
  static_analysis:
    - name: "Type checking"
      command: "mypy {src_dir} --ignore-missing-imports"
      timeout: 60
      pass_condition: "exit_code == 0"
      severity: "warning"  # warn but don't fail
    
    - name: "Security scan"
      command: "bandit -r {src_dir} -f json"
      timeout: 60
      pass_condition: "no_high_severity_issues"
      severity: "error"
    
    - name: "Lint check"
      command: "flake8 {src_dir} --max-line-length=100"
      timeout: 60
      pass_condition: "exit_code == 0"
      severity: "warning"
  
  file_inspection:
    - name: "Check log file exists"
      path: "{inspect_file}"
      condition: "exists and size > 0"
    
    - name: "Check coverage"
      path: "{coverage_report}"
      condition: "line_coverage >= {coverage_threshold}"

scoring:
  # All must pass for overall pass
  mode: "strict"
  weights:
    test_execution: 0.6
    static_analysis: 0.2
    file_inspection: 0.2

output:
  format: "json"
  fields:
    - overall_result: "PASS | FAIL"
    - details: "per-check results"
    - logs: "command outputs"